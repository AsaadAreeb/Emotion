{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Prepare Dataset\n",
    "\n",
    "## [0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tabulate import tabulate\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_classes=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, csv_data, transform, train=True):\n",
    "        self.data = pd.read_csv(csv_data)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if self.train:\n",
    "            pixels = self.data.iloc[idx, 1].split()\n",
    "            pixels = np.array(pixels, dtype=np.uint8).reshape(48, 48)\n",
    "    \n",
    "            image = Image.fromarray(pixels)\n",
    "    \n",
    "            label = int(self.data.iloc[idx, 0])\n",
    "    \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "    \n",
    "            return image, label\n",
    "\n",
    "        pixels = self.data.iloc[idx, 0].split()\n",
    "        pixels = np.array(pixels, dtype=np.uint8).reshape(48, 48)\n",
    "\n",
    "        image = Image.fromarray(pixels)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate All Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_TrainValTest_dataloaders(trpath='', tstpath='', batch_size=32):\n",
    "    Train_fer_dataset = FERDataset(csv_data=trpath, transform=transform)\n",
    "    \n",
    "    train_size = int(0.8 * len(Train_fer_dataset))\n",
    "    val_size = len(Train_fer_dataset) - train_size\n",
    "\n",
    "    Train_fer_dataset, Val_fer_dataset = random_split(Train_fer_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    Test_fer_dataset = FERDataset(csv_data=tstpath, transform=transform, train=False)\n",
    "\n",
    "    TrainDataLoader = DataLoader(Train_fer_dataset, batch_size=batch_size, shuffle=True)\n",
    "    ValDataLoader = DataLoader(Val_fer_dataset, batch_size=batch_size)\n",
    "    TestDataLoader = DataLoader(Test_fer_dataset, batch_size=batch_size)\n",
    "    return TrainDataLoader, ValDataLoader, TestDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train, val, test = generate_TrainValTest_dataloaders('data/train.csv', 'data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "emotion_dict = {\n",
    "    0:'Angry',\n",
    "    1:'Disgust',\n",
    "    2:'Fear',\n",
    "    3:'Happy',\n",
    "    4:'Sad',\n",
    "    5:'Surprise',\n",
    "    6:'Neutral',\n",
    "}\n",
    "\n",
    "emotion_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {emotion_dict[label.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class SimpleEmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleEmotionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 6 * 6, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(ValDataLoader, desc='Evaluating', leave=False):\n",
    "                inputs, labels = batch\n",
    "                outputs = self(inputs)\n",
    "                predicted_labels = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "                y_true.extend(labels.tolist())\n",
    "                y_pred.extend(predicted_labels.tolist())\n",
    "    \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, target_names=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"], zero_division=1)\n",
    "    \n",
    "        return accuracy, report\n",
    "\n",
    "    def predict(self, input_data, emotion_mapping):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generate predictions based on input_data\n",
    "            predictions = self(input_data)\n",
    "        \n",
    "            # Map numerical predictions to emotion labels\n",
    "            predicted_labels = torch.argmax(predictions, dim=1)\n",
    "            emotion_predictions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
    "    \n",
    "            return emotion_predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "simpleCNN = SimpleEmotionCNN(num_classes)\n",
    "\n",
    "# Define loss function and optimize\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "optimizer = optim.SGD(simpleCNN.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    simpleCNN.train()\n",
    "    for batch in tqdm(TrainDataLoader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = simpleCNN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    average_loss = total_loss / len(TrainDataLoader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': simpleCNN.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "}, 'model_checkpoint_50.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "\n",
    "simpleCNN = SimpleEmotionCNN(num_classes)\n",
    "\n",
    "simpleCNN.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "accuracy, report = simpleCNN.evaluate(ValDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "report_lines = report.split('\\n')\n",
    "# Print each line with proper formatting\n",
    "for line in report_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Image for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(48, 48)):\n",
    "\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "\n",
    "    image_tensor = torch.Tensor([image])\n",
    "    image_tensor = image_tensor.view(1, 1, 48, 48)\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_path = 'F:\\Class-BSSE\\pp.jpeg'\n",
    "image_path1 = 'F:\\Class-BSSE\\IMG_20210513_073359.jpg'\n",
    "image_path2 = 'F:\\Class-BSSE\\IMG_20210513_073354.jpg'\n",
    "image_path3 = 'myface.PNG'\n",
    "image_path4 = 'myface1.PNG'\n",
    "image_path4 = 'shaggy.PNG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_image = preprocess_image(image_path)\n",
    "\n",
    "image_array = preprocessed_image.squeeze().numpy()\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "emotion_mapping = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "emotion = simpleCNN.predict(preprocessed_image, emotion_mapping)\n",
    "emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images_with_predictions(images, predictions, class_names):\n",
    "    num_images = len(images)\n",
    "    \n",
    "    # Create a grid to display images and predictions\n",
    "    num_cols = 4  # You can adjust the number of columns as needed\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 3*num_rows))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_images:\n",
    "            image = images[i].numpy().squeeze()\n",
    "            predicted_probs = predictions[i].cpu().numpy()\n",
    "            top3_indices = np.argsort(predicted_probs)[::-1][:3]\n",
    "            \n",
    "            ax.imshow(image, cmap='gray')\n",
    "            ax.set_title('Top 3 Predictions:')\n",
    "            for j, idx in enumerate(top3_indices):\n",
    "                ax.set_title(f'{ax.get_title()}\\n{j+1}: {class_names[idx]} ({predicted_probs[idx]:.2f})')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Testing loop with image plotting\n",
    "simpleCNN.eval()  # Set the model to evaluation mode\n",
    "class_names = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in TestDataLoader:\n",
    "        outputs = simpleCNN(batch)  # Assuming batch contains only images\n",
    "\n",
    "        # Plot images with top 3 predictions for this batch\n",
    "        plot_images_with_predictions(batch, outputs, class_names)\n",
    "        \n",
    "        # Optionally, ask if the user wants to see the next batch\n",
    "        next_batch = input(\"Show the next batch of images? (y/n): \")\n",
    "        if next_batch.lower() != 'y':\n",
    "            break  # Exit the loop if the user doesn't want to see the next batch\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
