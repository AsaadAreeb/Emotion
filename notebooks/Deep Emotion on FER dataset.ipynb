{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Emotion Architecture\n",
    "class DeepEmotion(nn.Module):\n",
    "    def __init__(self, num_classes, regularization_lambda=0.001):\n",
    "        super(DeepEmotion, self).__init__()\n",
    "\n",
    "        self.regularization_lambda = regularization_lambda\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3)\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(810, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        \n",
    "\n",
    "        self.local_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.local_fc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3*2),\n",
    "        )\n",
    "\n",
    "        self.local_fc[2].weight.data.zero_()\n",
    "        self.local_fc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.local_net(x)\n",
    "        xs = xs.view(-1, 640)       # 10 * 3 * 3\n",
    "        theta = self.local_fc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        # grid = F.affine_grid(theta, x.size(), align_corners=True)\n",
    "        # x = F.grid_sample(x, grid, align_corners=True)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def calculate_regularization_loss(self):\n",
    "        # Calculate the L2 regularization loss for the weights in the last two fully-connected layers\n",
    "        regularization_loss = 0.0\n",
    "        for param in self.fc1.parameters():\n",
    "            regularization_loss += torch.norm(param, p=2)  # L2 norm\n",
    "        for param in self.fc2.parameters():\n",
    "            regularization_loss += torch.norm(param, p=2)  # L2 norm\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def compute_loss(self, outputs, labels):\n",
    "        # Cross-entropy loss\n",
    "        classification_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # L2 regularization loss\n",
    "        # regularization_loss = self.calculate_regularization_loss()\n",
    "\n",
    "        # Total loss with regularization\n",
    "        # total_loss = classification_loss + self.regularization_lambda * regularization_loss\n",
    "\n",
    "        return classification_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.stn(x)\n",
    "        localization_grid_resized = F.interpolate(grid, size=(9, 9), mode='bilinear', align_corners=False)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.maxpool2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.maxpool4(x))\n",
    "\n",
    "        x = F.dropout(x)\n",
    "\n",
    "        x = x * localization_grid_resized\n",
    "\n",
    "        x = x.view(-1, 810)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, csv_data, transform, train=True):\n",
    "        self.data = pd.read_csv(csv_data)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if self.train:\n",
    "            pixels = self.data.iloc[idx, 1].split()\n",
    "            pixels = np.array(pixels, dtype=np.uint8).reshape(48, 48)\n",
    "    \n",
    "            image = Image.fromarray(pixels)\n",
    "    \n",
    "            label = int(self.data.iloc[idx, 0])\n",
    "    \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "    \n",
    "            return image, label\n",
    "\n",
    "        pixels = self.data.iloc[idx, 0].split()\n",
    "        pixels = np.array(pixels, dtype=np.uint8).reshape(48, 48)\n",
    "\n",
    "        image = Image.fromarray(pixels)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_TrainValTest_dataloaders(trpath='', tstpath='', batch_size=32):\n",
    "    Train_fer_dataset = FERDataset(csv_data=trpath, transform=transform)\n",
    "    \n",
    "    train_size = int(0.8 * len(Train_fer_dataset))\n",
    "    val_size = len(Train_fer_dataset) - train_size\n",
    "\n",
    "    Train_fer_dataset, Val_fer_dataset = random_split(Train_fer_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    Test_fer_dataset = FERDataset(csv_data=tstpath, transform=transform, train=False)\n",
    "\n",
    "    TrainDataLoader = DataLoader(Train_fer_dataset, batch_size=batch_size, shuffle=True)\n",
    "    ValDataLoader = DataLoader(Val_fer_dataset, batch_size=batch_size)\n",
    "    TestDataLoader = DataLoader(Test_fer_dataset, batch_size=batch_size)\n",
    "    return TrainDataLoader, ValDataLoader, TestDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {\n",
    "    0:'Angry',\n",
    "    1:'Disgust',\n",
    "    2:'Fear',\n",
    "    3:'Happy',\n",
    "    4:'Sad',\n",
    "    5:'Surprise',\n",
    "    6:'Neutral',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Images with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {emotion_dict[label.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(epochs, trainloader, valloader, optimizer, model):\n",
    "    print(\"===================================Start Training===================================\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for data, labels in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{epochs} (Training)\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = model.compute_loss(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        for data, labels in tqdm(valloader, desc=f\"Epoch {epoch + 1}/{epochs} (Validation)\"):\n",
    "            val_outputs = model(data)\n",
    "\n",
    "            val_loss = model.compute_loss(val_outputs, labels)\n",
    "\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        validation_loss = validation_loss / len(val_loader)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        \n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Accuracy {:.3f}% \\tValidation Accuracy {:.3f}%'\n",
    "              .format(epoch + 1, train_loss, validation_loss, train_acc * 100, val_acc * 100))\n",
    "\n",
    "    torch.save(model.state_dict(), 'deep_emotion-{}.pt'.format(epochs))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "num_classes = 7\n",
    "epochs = 50\n",
    "train_loader, val_loader, test_loader = generate_TrainValTest_dataloaders('data/train.csv', 'data/test.csv')\n",
    "model = DeepEmotion(num_classes=num_classes, regularization_lambda=0.001)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "Train(epochs, train_loader, val_loader, optimizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to break down line by line and output each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "model = DeepEmotion(num_classes)\n",
    "model.load_state_dict(torch.load('checkpoints/deep_emotion-100.pt', map_location=torch.device('cpu')))\n",
    "frame = cv2.imread('veryangryface.jpeg')\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "for x, y, w, h in faces:\n",
    "    roi_gray = gray[y:y + h, x:x + w]\n",
    "    roi_color = frame[y:y + h, x:x + w]\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    facesC = faceCascade.detectMultiScale(roi_gray)\n",
    "    if len(facesC) == 0:\n",
    "        print(\"Face not detected!\")\n",
    "    else:\n",
    "        print(\"Found Face!\")\n",
    "        for (ex, ey, ew, eh) in facesC:\n",
    "            face_roi = roi_color[ey:ey + eh, ex:ex + ew]\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(face_roi)\n",
    "face_roi.shape\n",
    "gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "gray.shape\n",
    "final_image = cv2.resize(gray, (48, 48))\n",
    "final_image.shape\n",
    "final_image = np.expand_dims(final_image, axis=0)\n",
    "final_image.shape\n",
    "final_image = np.expand_dims(final_image, axis=0)\n",
    "final_image.shape\n",
    "final_image = final_image/255.0\n",
    "torchTensor = torch.from_numpy(final_image)\n",
    "torchTensor = torchTensor.type(torch.FloatTensor)\n",
    "output = model(torchTensor)\n",
    "pred = F.softmax(output, dim=1)\n",
    "print(pred)\n",
    "index_pred = torch.argmax(pred)\n",
    "emotion_dict[index_pred.item()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiveCam demo\n",
    "# Load the Haar Cascade classifier for face detection\n",
    "path = 'haarcascade_frontalface_default.xml'\n",
    "faceCascade = cv2.CascadeClassifier(path)\n",
    "\n",
    "# Initialize the model for emotion recognition (replace 'model' with your actual model)\n",
    "num_classes = 7\n",
    "model = DeepEmotion(num_classes)\n",
    "model.load_state_dict(torch.load('checkpoints/deep_emotion-100.pt', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam, change to 1 if you have an additional camera\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Face detection\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Emotion recognition (process the ROI)\n",
    "        gray_face = cv2.cvtColor(roi_color, cv2.COLOR_BGR2GRAY)\n",
    "        final_image = cv2.resize(gray_face, (48, 48))\n",
    "        final_image = np.expand_dims(final_image, axis=0)\n",
    "        final_image = np.expand_dims(final_image, axis=0)\n",
    "        final_image = final_image / 255.0\n",
    "\n",
    "        torchTensor = torch.from_numpy(final_image)\n",
    "        torchTensor = torchTensor.type(torch.FloatTensor)\n",
    "\n",
    "        output = model(torchTensor)\n",
    "        pred = F.softmax(output, dim=1)\n",
    "\n",
    "        index_pred = torch.argmax(pred)\n",
    "\n",
    "        # Draw bounding box around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        # Draw emotion label on the frame\n",
    "        emotion_label = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "        emotion = emotion_label[index_pred.item()]\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
